# This CITATION.cff file was generated with cffinit.
# Visit https://bit.ly/cffinit to generate yours today!

cff-version: 1.2.0
title: One-Shot Labeling for Automatic Relevance Estimation
message: >-
  If you use this software, please cite it using the
  metadata from this file.
type: article
authors:
  - given-names: Sean
    family-names: MacAvaney
    email: sean.macavaney@glasgow.ac.uk
    affiliation: University of Glasgow
    orcid: 'https://orcid.org/0000-0002-8914-2659'
  - given-names: Luca
    family-names: Soldaini
    affiliation: Allen Institute for AI
    orcid: 'https://orcid.org/0000-0001-6998-9863'
    email: lucas@allenai.org
identifiers:
  - type: url
    value: 'https://arxiv.org/abs/2302.11266'
    description: ArXiv
  - type: doi
    value: 10.1145/3539618.3592032
repository-code: 'https://github.com/seanmacavaney/autoqrels'
abstract: >-
  Dealing with unjudged documents ("holes") in relevance
  assessments is a perennial problem when evaluating search
  systems with offline experiments. Holes can reduce the
  apparent effectiveness of retrieval systems during
  evaluation and introduce biases in models trained with
  incomplete data. In this work, we explore whether large
  language models can help us fill such holes to improve
  offline evaluations. We examine an extreme, albeit common,
  evaluation setting wherein only a single known relevant
  document per query is available for evaluation. We then
  explore various approaches for predicting the relevance of
  unjudged documents with respect to a query and the known
  relevant document, including nearest neighbor, supervised,
  and prompting techniques. We find that although the
  predictions of these One-Shot Labelers (1SL) frequently
  disagree with human assessments, the labels they produce
  yield a far more reliable ranking of systems than the
  single labels do alone. Specifically, the strongest
  approaches can consistently reach system ranking
  correlations of over 0.86 with the full rankings over a
  variety of measures. Meanwhile, the approach substantially
  increases the reliability of t-tests due to filling holes
  in relevance assessments, giving researchers more
  confidence in results they find to be significant. 

  Alongside this work, we release an easy-to-use software
  package to enable the use of 1SL for evaluation of other
  ad-hoc collections or systems.
license: Apache-2.0
